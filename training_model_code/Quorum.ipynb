{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Project Quorum - Lightweight Log Anomaly Detection Model Training\n",
        "Optimized for <1.5GB RAM usage on Google Colab\n",
        "Supports both PyOD and TensorFlow Lite models\n",
        "\n"
      ],
      "metadata": {
        "id": "drwUqmDhmsVr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FI7h6QlHmgiX",
        "outputId": "de560d32-ae0c-44a1-839a-5368268d6755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pyod in /usr/local/lib/python3.12/dist-packages (2.0.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from pyod) (3.10.0)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.12/dist-packages (from pyod) (0.60.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51->pyod) (0.43.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->pyod) (3.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn pyod joblib tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "_bBxtLUPpdyx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # Dataset URLs (LogPai LogHub) - Updated to raw URLs\n",
        "    'datasets': [\n",
        "        'https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_2k.log_structured.csv',\n",
        "        'https://raw.githubusercontent.com/logpai/loghub/master/BGL/BGL_2k.log_structured.csv',\n",
        "        'https://raw.githubusercontent.com/logpai/loghub/master/Thunderbird/Thunderbird_2k.log_structured.csv',\n",
        "        'https://raw.githubusercontent.com/logpai/loghub/master/Mac/Mac_2k.log_structured.csv',\n",
        "        'https://raw.githubusercontent.com/logpai/loghub/master/Windows/Windows_2k.log_structured.csv',\n",
        "        'https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log_structured.csv',\n",
        "    ],\n",
        "\n",
        "    # Model settings\n",
        "    'max_features': 1000,  # Reduced for memory efficiency\n",
        "    'contamination': 0.02,  # Expected anomaly rate (2%)\n",
        "    'chunk_size': 500,      # Process data in chunks to save memory\n",
        "    'model_type': 'pyod',   # 'pyod' or 'tflite'\n",
        "}"
      ],
      "metadata": {
        "id": "wvtUBX8bnNu6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: DATA LOADING (Memory-Efficient)\n",
        "# ============================================================================\n",
        "\n",
        "def load_dataset_from_url(url, sample_size=None):\n",
        "    \"\"\"Load dataset directly from GitHub URL with memory optimization\"\"\"\n",
        "    print(f\"üì• Loading dataset from: {url}\")\n",
        "\n",
        "    try:\n",
        "        # Read CSV with minimal columns to save memory\n",
        "        df = pd.read_csv(url, usecols=['Content', 'Label'] if 'Label' in pd.read_csv(url, nrows=1).columns else ['Content'])\n",
        "\n",
        "        # Sample if dataset is too large\n",
        "        if sample_size and len(df) > sample_size:\n",
        "            df = df.sample(n=sample_size, random_state=42)\n",
        "            print(f\"‚ö†Ô∏è Sampled {sample_size} rows for memory efficiency\")\n",
        "\n",
        "        print(f\"‚úÖ Loaded {len(df)} rows\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def prepare_data(datasets_urls, sample_per_dataset=5000):\n",
        "    \"\"\"Load and combine multiple datasets efficiently\"\"\"\n",
        "    all_messages = []\n",
        "    all_labels = []\n",
        "\n",
        "    for url in datasets_urls:\n",
        "        df = load_dataset_from_url(url, sample_size=sample_per_dataset)\n",
        "        if df is not None:\n",
        "            all_messages.extend(df['Content'].fillna('').tolist())\n",
        "\n",
        "            # Handle labeled vs unlabeled data\n",
        "            if 'Label' in df.columns:\n",
        "                all_labels.extend(df['Label'].map({'Normal': 0, 'Anomaly': 1}).fillna(0).tolist())\n",
        "            else:\n",
        "                all_labels.extend([0] * len(df))  # Assume normal if no labels\n",
        "\n",
        "    return pd.DataFrame({'message': all_messages, 'label': all_labels})\n"
      ],
      "metadata": {
        "id": "70tSpNPUoTyG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: FEATURE EXTRACTION (TF-IDF with Memory Optimization)\n",
        "# ============================================================================\n",
        "\n",
        "def extract_features(messages, max_features=1000):\n",
        "    \"\"\"Convert log messages to numerical features using TF-IDF\"\"\"\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "    print(f\"üîß Extracting features (max_features={max_features})...\")\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=max_features,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2),\n",
        "        max_df=0.95,\n",
        "        min_df=2\n",
        "    )\n",
        "\n",
        "    X = vectorizer.fit_transform(messages)\n",
        "    print(f\"‚úÖ Feature matrix shape: {X.shape}\")\n",
        "    print(f\"üíæ Memory usage: ~{X.data.nbytes / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "    return X, vectorizer"
      ],
      "metadata": {
        "id": "U2_uKNjlobuv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3A: TRAIN PYOD MODEL (Lightweight Anomaly Detection)\n",
        "# ============================================================================\n",
        "\n",
        "def train_pyod_model(X, contamination=0.02):\n",
        "    \"\"\"Train Isolation Forest using PyOD (very memory efficient)\"\"\"\n",
        "    from pyod.models.iforest import IForest\n",
        "\n",
        "    print(f\"ü§ñ Training PyOD Isolation Forest...\")\n",
        "    print(f\"   - Contamination: {contamination}\")\n",
        "    print(f\"   - Estimators: 100\")\n",
        "\n",
        "    model = IForest(\n",
        "        contamination=contamination,\n",
        "        n_estimators=100,\n",
        "        max_samples='auto',\n",
        "        random_state=42,\n",
        "        n_jobs=1  # Single thread to save memory\n",
        "    )\n",
        "\n",
        "    # Convert sparse to dense only if needed (IForest works with dense)\n",
        "    if hasattr(X, 'toarray'):\n",
        "        X_dense = X.toarray()\n",
        "    else:\n",
        "        X_dense = X\n",
        "\n",
        "    model.fit(X_dense)\n",
        "    print(\"‚úÖ Model trained successfully!\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "2pKMT4XKoh9g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3B: TRAIN TFLITE MODEL (Neural Network Autoencoder)\n",
        "# ============================================================================\n",
        "\n",
        "def train_tflite_model(X, epochs=5):\n",
        "    \"\"\"Train autoencoder and convert to TFLite\"\"\"\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "\n",
        "    print(f\"ü§ñ Training TensorFlow Lite Autoencoder...\")\n",
        "\n",
        "    # Convert sparse to dense\n",
        "    if hasattr(X, 'toarray'):\n",
        "        X_dense = X.toarray().astype(np.float32)\n",
        "    else:\n",
        "        X_dense = X.astype(np.float32)\n",
        "\n",
        "    input_dim = X_dense.shape[1]\n",
        "\n",
        "    # Simple autoencoder architecture (memory efficient)\n",
        "    autoencoder = keras.Sequential([\n",
        "        keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(32, activation='relu'),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(input_dim, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    autoencoder.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='mse'\n",
        "    )\n",
        "\n",
        "    print(f\"   - Training for {epochs} epochs...\")\n",
        "    autoencoder.fit(\n",
        "        X_dense, X_dense,\n",
        "        epochs=epochs,\n",
        "        batch_size=128,\n",
        "        validation_split=0.1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Model trained!\")\n",
        "    return autoencoder\n",
        "\n",
        "def convert_to_tflite(model):\n",
        "    \"\"\"Convert Keras model to TFLite format\"\"\"\n",
        "    import tensorflow as tf\n",
        "\n",
        "    print(\"üì¶ Converting to TensorFlow Lite...\")\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Quantization\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    print(f\"‚úÖ TFLite model size: {len(tflite_model) / 1024:.2f} KB\")\n",
        "    return tflite_model"
      ],
      "metadata": {
        "id": "5XUGWkwhokw7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 4: MODEL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, X, y_true, model_type='pyod'):\n",
        "    \"\"\"Quick evaluation of trained model\"\"\"\n",
        "    from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "    print(\"\\nüìä Model Evaluation:\")\n",
        "\n",
        "    if model_type == 'pyod':\n",
        "        # Get anomaly scores\n",
        "        scores = model.decision_function(X.toarray() if hasattr(X, 'toarray') else X)\n",
        "        predictions = model.predict(X.toarray() if hasattr(X, 'toarray') else X)\n",
        "    else:\n",
        "        # For autoencoder: reconstruction error\n",
        "        X_dense = X.toarray() if hasattr(X, 'toarray') else X\n",
        "        reconstructed = model.predict(X_dense)\n",
        "        scores = np.mean(np.square(X_dense - reconstructed), axis=1)\n",
        "        threshold = np.percentile(scores, 98)\n",
        "        predictions = (scores > threshold).astype(int)\n",
        "\n",
        "    # Print metrics\n",
        "    print(classification_report(y_true, predictions, target_names=['Normal', 'Anomaly']))\n",
        "\n",
        "    if len(np.unique(y_true)) > 1:\n",
        "        auc = roc_auc_score(y_true, scores)\n",
        "        print(f\"ROC-AUC Score: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "HFGTKLFaonYH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: SAVE MODELS\n",
        "# ============================================================================\n",
        "\n",
        "def save_pyod_model(model, vectorizer, output_dir='./'):\n",
        "    \"\"\"Save PyOD model and vectorizer\"\"\"\n",
        "    print(\"\\nüíæ Saving PyOD model...\")\n",
        "\n",
        "    joblib.dump(model, f'{output_dir}/iforest_model.pkl')\n",
        "    joblib.dump(vectorizer, f'{output_dir}/tfidf_vectorizer.pkl')\n",
        "\n",
        "    print(f\"‚úÖ Saved:\")\n",
        "    print(f\"   - {output_dir}/iforest_model.pkl\")\n",
        "    print(f\"   - {output_dir}/tfidf_vectorizer.pkl\")\n",
        "\n",
        "def save_tflite_model(tflite_model, vectorizer, output_dir='./'):\n",
        "    \"\"\"Save TFLite model and vectorizer\"\"\"\n",
        "    print(\"\\nüíæ Saving TFLite model...\")\n",
        "\n",
        "    with open(f'{output_dir}/anomaly_detector.tflite', 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    joblib.dump(vectorizer, f'{output_dir}/tfidf_vectorizer.pkl')\n",
        "\n",
        "    print(f\"‚úÖ Saved:\")\n",
        "    print(f\"   - {output_dir}/anomaly_detector.tflite\")\n",
        "    print(f\"   - {output_dir}/tfidf_vectorizer.pkl\")"
      ],
      "metadata": {
        "id": "zfTGybP8oqOK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: INFERENCE TEST\n",
        "# ============================================================================\n",
        "\n",
        "def test_inference(model, vectorizer, model_type='pyod'):\n",
        "    \"\"\"Test model with sample log messages\"\"\"\n",
        "    print(\"\\nüß™ Testing Inference:\")\n",
        "\n",
        "    test_logs = [\n",
        "        \"User root logged in successfully\",\n",
        "        \"Connection established from 192.168.1.1\",\n",
        "        \"CRITICAL: Segmentation fault in sshd\",\n",
        "        \"Failed password attempt for admin\",\n",
        "        \"System backup completed successfully\"\n",
        "    ]\n",
        "\n",
        "    X_test = vectorizer.transform(test_logs)\n",
        "\n",
        "    if model_type == 'pyod':\n",
        "        X_test_dense = X_test.toarray()\n",
        "        scores = model.decision_function(X_test_dense)\n",
        "        predictions = model.predict(X_test_dense)\n",
        "    else:\n",
        "        X_test_dense = X_test.toarray()\n",
        "        reconstructed = model.predict(X_test_dense)\n",
        "        scores = np.mean(np.square(X_test_dense - reconstructed), axis=1)\n",
        "        predictions = (scores > np.percentile(scores, 80)).astype(int)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    for log, score, pred in zip(test_logs, scores, predictions):\n",
        "        status = \"üö® ANOMALY\" if pred == 1 else \"‚úÖ NORMAL\"\n",
        "        print(f\"{status} | Score: {score:.4f} | {log[:50]}...\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "79V27PI1orDx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN TRAINING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"üéØ PROJECT QUORUM - LOG ANOMALY DETECTION TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Step 1: Load data\n",
        "    print(\"\\nüìÇ STEP 1: Loading datasets...\")\n",
        "    df = prepare_data(CONFIG['datasets'], sample_per_dataset=5000)\n",
        "    print(f\"‚úÖ Total samples: {len(df)}\")\n",
        "\n",
        "    # Step 2: Extract features\n",
        "    print(\"\\nüîß STEP 2: Feature extraction...\")\n",
        "    X, vectorizer = extract_features(df['message'], CONFIG['max_features'])\n",
        "\n",
        "    # Step 3: Train model\n",
        "    print(f\"\\nü§ñ STEP 3: Training {CONFIG['model_type'].upper()} model...\")\n",
        "\n",
        "    if CONFIG['model_type'] == 'pyod':\n",
        "        model = train_pyod_model(X, CONFIG['contamination'])\n",
        "\n",
        "        # Evaluate\n",
        "        evaluate_model(model, X, df['label'], 'pyod')\n",
        "\n",
        "        # Save\n",
        "        save_pyod_model(model, vectorizer)\n",
        "\n",
        "        # Test\n",
        "        test_inference(model, vectorizer, 'pyod')\n",
        "\n",
        "    else:  # tflite\n",
        "        model = train_tflite_model(X, epochs=5)\n",
        "        tflite_model = convert_to_tflite(model)\n",
        "\n",
        "        # Evaluate\n",
        "        evaluate_model(model, X, df['label'], 'tflite')\n",
        "\n",
        "        # Save\n",
        "        save_tflite_model(tflite_model, vectorizer)\n",
        "\n",
        "        # Note: TFLite inference test requires TFLite interpreter\n",
        "        print(\"\\n‚ö†Ô∏è TFLite inference test skipped (use TFLite interpreter in production)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "    print(\"üì¶ Download the .pkl/.tflite files and integrate into your backend\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "3nUjHAAbouFh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# RUN TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required packages first (run in Colab cell):\n",
        "    # !pip install pandas numpy scikit-learn pyod joblib tensorflow\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k891_GIo4Lm",
        "outputId": "06c12e37-796d-4760-af0a-93b9846b25d2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üéØ PROJECT QUORUM - LOG ANOMALY DETECTION TRAINING\n",
            "======================================================================\n",
            "\n",
            "üìÇ STEP 1: Loading datasets...\n",
            "üì• Loading dataset from: https://raw.githubusercontent.com/logpai/loghub/master/HDFS/HDFS_2k.log_structured.csv\n",
            "‚úÖ Loaded 2000 rows\n",
            "üì• Loading dataset from: https://raw.githubusercontent.com/logpai/loghub/master/BGL/BGL_2k.log_structured.csv\n",
            "‚úÖ Loaded 2000 rows\n",
            "üì• Loading dataset from: https://raw.githubusercontent.com/logpai/loghub/master/Thunderbird/Thunderbird_2k.log_structured.csv\n",
            "‚úÖ Loaded 2000 rows\n",
            "üì• Loading dataset from: https://raw.githubusercontent.com/logpai/loghub/master/Mac/Mac_2k.log_structured.csv\n",
            "‚úÖ Loaded 2000 rows\n",
            "üì• Loading dataset from: https://raw.githubusercontent.com/logpai/loghub/master/Windows/Windows_2k.log_structured.csv\n",
            "‚úÖ Loaded 2000 rows\n",
            "üì• Loading dataset from: https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log_structured.csv\n",
            "‚úÖ Loaded 2000 rows\n",
            "‚úÖ Total samples: 12000\n",
            "\n",
            "üîß STEP 2: Feature extraction...\n",
            "üîß Extracting features (max_features=1000)...\n",
            "‚úÖ Feature matrix shape: (12000, 1000)\n",
            "üíæ Memory usage: ~0.93 MB\n",
            "\n",
            "ü§ñ STEP 3: Training PYOD model...\n",
            "ü§ñ Training PyOD Isolation Forest...\n",
            "   - Contamination: 0.02\n",
            "   - Estimators: 100\n",
            "‚úÖ Model trained successfully!\n",
            "\n",
            "üìä Model Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      0.98      0.99     12000\n",
            "     Anomaly       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.98     12000\n",
            "   macro avg       0.50      0.49      0.50     12000\n",
            "weighted avg       1.00      0.98      0.99     12000\n",
            "\n",
            "\n",
            "üíæ Saving PyOD model...\n",
            "‚úÖ Saved:\n",
            "   - .//iforest_model.pkl\n",
            "   - .//tfidf_vectorizer.pkl\n",
            "\n",
            "üß™ Testing Inference:\n",
            "\n",
            "======================================================================\n",
            "‚úÖ NORMAL | Score: -0.0514 | User root logged in successfully...\n",
            "‚úÖ NORMAL | Score: -0.0508 | Connection established from 192.168.1.1...\n",
            "‚úÖ NORMAL | Score: -0.0502 | CRITICAL: Segmentation fault in sshd...\n",
            "‚úÖ NORMAL | Score: -0.0508 | Failed password attempt for admin...\n",
            "‚úÖ NORMAL | Score: -0.0532 | System backup completed successfully...\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "‚úÖ TRAINING COMPLETE!\n",
            "üì¶ Download the .pkl/.tflite files and integrate into your backend\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q7stIrP8pjbe"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}